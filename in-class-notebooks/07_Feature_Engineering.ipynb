{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Feature Engineering \n",
    "***\n",
    "\n",
    "<img src=\"figs/logregwordcloud.png\",width=1100,height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**:  Go to the botttom of the notebook and shift-enter the [helper functions](#helpers)\n",
    "***\n",
    "\n",
    "The success of a machine learning algorithm on a predictive problem is highly dependent on the way you as the practitioner present the data. Consider the following example. \n",
    "\n",
    "### Problem 1: Intuition\n",
    "***\n",
    "Suppose that you want to train a model to predict whether it is possible to drive between two cities in a single day.  The raw data includes the latitude and longitude of the two cities and the training data is labeled with $\\texttt{Yes}$ for is drivable and $\\texttt{No}$ for is not drivable.  One particular training set my look like \n",
    "\n",
    "|$\\texttt{CITY 1 LAT.}$ | $\\texttt{CITY 1 LNG.}$ |$\\texttt{CITY 2 LAT.}$ | $\\texttt{CITY 2 LNG.}$ | $\\texttt{DRIVABLE}$? | \n",
    "|:----:|:----:|:----:|:----:|:----:|\n",
    "| 123.24 |46.71\t| 121.33| 47.34\t| Yes |\n",
    "|123.24\t|56.91\t|121.33\t|55.23\t|Yes |\n",
    "|123.24\t|46.71\t|121.33\t|55.34\t|No |\n",
    "|123.24\t|46.71\t|130.99\t|47.34\t|No |\n",
    "\n",
    "**Q**: Given the following features, do you expect a linear classifier like Logistic Regression to be successful? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What features could you create that *would* be correlated with the correct classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Could we come up with a process to do this automatically?  Next question:  should we come up with a process to do this automatically? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 2: Transformations on Continuous Data \n",
    "***\n",
    "\n",
    "Consider the case when you're trying to model quality of life of a person from survey data which asks a multitude of things like $\\texttt{income}$, $\\texttt{education level}$, $\\texttt{num children}$, etc.  As part of data exploration, you might try plotting the distributions of the raw data by feature individually.  A histogram of a potential data set for income might look as follows:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_inc = income_data()\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "n, bins, patches = plt.hist(x_inc, 15, normed=1, facecolor=mycolors[\"red\"], alpha=0.75)\n",
    "plt.xlabel(\"income in tens of thousands\", fontsize=16)\n",
    "plt.ylabel(\"frequency\", fontsize=16);\n",
    "plt.title(\"Income Frequency\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like we would expect for income.  There are lots of people at the low end of the spectrum, a few more in the middle, and very few in the high income categories.  \n",
    "\n",
    "There are two things to consider here.  First, lots of ML models assume that your input features are generally normally distributed.  Second, from a feature engineering perspective, we can think about how much income actually affects happiness.  It seems reasonable to believe that once you get to a certain level, increasing income has a diminishing effect on happiness.  \n",
    "\n",
    "Both of these viewpoints motivate us to try a lot transformation on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_inc_log = np.log(x_inc)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "n, bins, patches = plt.hist(x_inc_log, 15, normed=1, facecolor=mycolors[\"blue\"], alpha=0.75)\n",
    "plt.xlabel(\"log(income in thousands)\", fontsize=16)\n",
    "plt.ylabel(\"frequency\", fontsize=16);\n",
    "plt.title(\"Income Frequency\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a distribution that looks fairly Gaussian, which might perform better in an machine learning model. \n",
    "\n",
    "### Problem 3: Categorical Data \n",
    "*** \n",
    "\n",
    "Encoding categorical data can be tricky.  Consider the case when you have a raw feature in your model that corresponds to a person's hair color.  Possible values might be $\\texttt{blonde}$, $\\texttt{brunette}$, $\\texttt{redhead}$.  How should we encode these as numerical models in a ML algorithm? \n",
    "\n",
    "A natural (but misleading) thing to do would be to assign an integer to each possible value of the feature.  For instance, we could do \n",
    "\n",
    "|Religion| Feature|\n",
    "|:-------:|:------:|\n",
    "|blonde| 0 |\n",
    "|brunette | 1 | \n",
    "|redhead | 2|\n",
    "\n",
    "**Q**:  What is potentially wrong with this encoding, particularly in a regression context? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "\n",
    "### Problem 4: Temporal Data \n",
    "*** \n",
    "\n",
    "Suppose that you're trying to train a model to predict that amount of foot-traffic at the 29th Street Mall.  Mall managers might be interested in such a model to predict the amount of janitorial and security services they need to employ at different times.  \n",
    "\n",
    "Suppose your training data consists of measurements of the amount of foot traffic at the mall and the date/time stamps that they were measured.  For instance, a training set might look like \n",
    "\n",
    "|$\\texttt{date_time_stamp}$| $\\texttt{FootTraffic}$|\n",
    "|:-------:|:------:|\n",
    "|$\\texttt{2015-11-12-20:00}$| 70|\n",
    "|$\\texttt{2015-06-10-21:00}$| 100|\n",
    "|$\\texttt{2015-08-02-12:00}$| 120|\n",
    "|$\\texttt{2015-12-22-12:00}$| 20|\n",
    "\n",
    "\n",
    "**Q**: How might you create meaningful features on the $\\texttt{date_time_stamp}$ data that would be useful for prediction? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 5: Part-of-Speech-Tagging\n",
    "*This problem and associated code was adapted from Jordan Boyd-Graber*\n",
    "***\n",
    "\n",
    "In computational linguistics, part-of-speech tagging (POST) is the process of marking a word in a text as a particular part of speech (e.g. noun, verb, adjective, etc), based on both its definition and its context. \n",
    "\n",
    "In this problem we will work with the <a href=\"https://en.wikipedia.org/wiki/Brown_Corpus\">Brown Corpus</a>, a compilation of 500 samples of English-language text totaling over a million words. The Brown Corpus is available through python's <a href=\"http://www.nltk.org/\">Natural Language Toolkit</a>.  Each word in the corpus has been tagged as: \n",
    "\n",
    "|type|symbol|\n",
    "|:--:|:----:|\n",
    "|adjective| JJ|\n",
    "|noun|NN|\n",
    "|pronoun|PP|\n",
    "|adverb|RB|\n",
    "|verb|VB|\n",
    "\n",
    "For the classification we will use simple Logistic Regression and focus on making iterative improvements by adding good features to our model.  The code for this problem is located in the helper functions section below.  Scroll down now and take a look at the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Baseline**: When starting to evaluate the usefulness of features, it is usually a good idea to create a baseline.  That is, run your model with little to no features and see how the model performs.  The following code will run logistic regression with only a bias feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_of_speech(limit=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now we know that simply making predictions based on the number of occurrences of each type of speech leads to a *training accuracy* of around 52% and a *validation accuracy* of around 56%.  Hopefully we can improve on this by actually giving the model some useful features.  \n",
    "\n",
    "The obvious choice is to actually tell the model what the words is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_of_speech(limit=500, word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that seems more reasonable.  Now you should have a training accuracy of around 96% and a validation accuracy of around 79%.  It looks like using the words alone as features induces some overfitting on the training set. Let's see if we can think of new features that we could include that might help.  \n",
    "\n",
    "Here is where you have to use your brain to do feature engineering!  \n",
    "\n",
    "The code has output some useful information that you can use to do error analysis and hopefully come up with some useful features.  \n",
    "\n",
    "The top of the output gives you examples of the features that were used for several examples.  Since we only used the words as features, each example only includes the word itself. \n",
    "\n",
    "The next useful piece of information is shown in the *confusion matrix*. Sci-Kit Learn's confusion matrix function returns a matric $C$ such that $C_{ij}$ gives the number of examples known to be in group $i$ that were labeled as group $j$.  From the confusion matrix in the output we see that, in particular, the model is classifying a lot of words that should be verbs as nouns.   \n",
    "\n",
    "Now, based on this knowledge you might have some ideas about new features you can include to combat this error, but maybe you don't?  It's almost always a good idea to dig into the actual data and look at *specific* examples that your model has misclassified.  To help you with this, the code has printed some of the most common misclassifications.  \n",
    "\n",
    "**Q**: Looking at the common misclassifications, can you think of a good new feature to add? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your new feature to the model and see how it does! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_of_speech(limit=500, word=True, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**:  Did your model do better on the test data?  Take a look at the confusion matrix and the examples of misclassifications and see if you can think of another new feature to add! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_of_speech(limit=500, word=True, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**:  What's your performance like now?  Repeat this iterative process of adding new features until you're happy with your model (or you've exhausted the number of features available in the code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_of_speech(limit=500, word=True, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 6: Building Multiple Features in SKLearn with Custom Transformers and FeatureUnion\n",
    "***\n",
    "\n",
    "In the Feature Engineering assignment your goal is to hand-craft features to predict whether statements about TV shows contain spoilers or not.  There are endless possibilities for useful features that you might want to try out, but for those of you not familiar with text-learning in sklearn just the process of getting up and running might seem daunting.  In this problem I will demonstrate the use of two particular objects that may make your life significantly easier: the generic Transformer and the FeatureUnion pipeline.  We will also make use of the CountVectorizer which is similar to the HashVectorizer seen in Problem 5.  If you have significant experience in text-learning then you have likely seen these things before (and/or know better things) and can safely skip this exercise. \n",
    "\n",
    "For the purpose of this discussion we will assume, like in the homework, that our data is a list of sentence strings.  For instance, we might have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [\n",
    "    \"The quick brown fox jumped over the lazy lazy dog\",\n",
    "    \"There is that dog and fox again\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to turn these data into a vectorized bag-of-word representation we can use an instance of CountVectorizer, which is a simple transformer that turns raw text into bag-or-words vectors.  To make the number of word-features more manageable I will call CountVectorizer with a list of stop words to be removed.  \n",
    "\n",
    "To transform the data into a matrix I simply call the $\\texttt{fit_transform}$ method on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words = CountVectorizer(stop_words='english')\n",
    "X = bag_of_words.fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to see the names of the specific features created (and important for our purpose, the order in which they're encoded) I can call the $\\texttt{get_feature_names}$ method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The named features are \", bag_of_words.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that CountVectorizer removed most of the common words like \"the\", \"there\", \"is\" and \"and\" and stores the important words in alphabetical order. \n",
    "\n",
    "Let's check the type and shape of the matrix produced by the transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"X has type \", type(X)\n",
    "print \"X has shape \", X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So CountVectorizer returns our vectorized features as a sparse matrix.  This makes sense since most feature vectors in text applications only contain a handful of vocabulary words and are therefore very sparse. \n",
    "\n",
    "Finally, the matrix X has two rows and six columns.  Each row corresponds to a training example (a sentence) and each column to a word-feature with the order corresponding to the list returned by $\\texttt{get_feature_names}$. \n",
    "\n",
    "Let's look at the matrix and see if it does what we think it's doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row of X should refer to the first sentence in $\\texttt{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first column refers to instances of \"brown\", and the fifth column refers to instances of \"lazy\".  OK, so there's an example of a simple canned transformer that we can apply to raw text-data.  But what if we want to craft something a little less standard?  What if, for instance, we're convinced that the number of times the characters \"x\", \"y\", and \"z\" appear in a sentence is somehow an important feature (probably not, but just go with it).  How could add these features to our data matrix?   \n",
    "\n",
    "There are definitely simple hacky ways to do this, but one slick way is to write your own custom transformer.  This transformer will take in raw text-data, turn them into numeric feature vectors (counts of the number of \"x\", \"y\", and \"z\"s) and return the matrix of transformed data.  One such transformer might look as follows  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class XYZTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        letters = ['x', 'y', 'z']\n",
    "         \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), len(letters)))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii,:] = np.array([x.count(letter) for letter in letters])\n",
    "            \n",
    "        return csr_matrix(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially we're writing a class that takes in the type of data we expect, transforms it the way we expect, and returns it the way we expect.  One thing to note is that by convention the transformer contains a $\\texttt{fit}$ method that only results $\\texttt{self}$.  All of the magic actually happens in the $\\texttt{transform}$ method. \n",
    "\n",
    "Let's test it out on our simple training data and see if it does what we expect.  Remember that the sentences in the training data are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train[0]\n",
    "print train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of the XYZTransformer \n",
    "xyz = XYZTransformer()\n",
    "\n",
    "# Fit it to our data \n",
    "Y = xyz.fit_transform(train)\n",
    "\n",
    "# Print a dense version of the matrix \n",
    "print Y.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first row (corresponding to the first sentence in the data) indicates that there is 1 x, 2 y's, and 2 z's in the first sentence, which you can verify is correct.  Similarly, the second row indicates that there is a single x (from \"fox\") in the sentence and no y's or z's. \n",
    "\n",
    "OK, so you've built your first transformer.  Now, let's say we want to combine the bag-of-words vectors and the letter features into a single data matrix.  We can do this with a particular class called a $\\texttt{FeatureUnion}$.  If you're familiar with sklearn's pipelines, know that a $\\texttt{FeatureUnion}$ is a pipeline specifically designed for transformer objects. \n",
    "\n",
    "Now we'll combine our two transformers into a mega-transformer that will zip all of our features into a nice package.  It might look as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "allmyfeatures = FeatureUnion([\n",
    "        (\"bag-of-words\", CountVectorizer(stop_words='english')),\n",
    "        (\"letter-counts\", XYZTransformer())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll call our mega-transformer on the original data, and hopefully get a sparse matrix out that encapsulates all of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = allmyfeatures.fit_transform(train)\n",
    "print \"Z has type \", type(Z)\n",
    "print \"Z has shape \", Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the combined data matrix Z is again a csr matrix (by design) and has two rows and nine columns.  The two rows again correspond to the two pieces of data in the train set, and the nine columns correspond to the six bag-of-word features from CountVectorizer and the three letter count features from XYZTransformer.  If we print a dense version of Z ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print Z.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will see that it looks exactly like the data matrices X and Y concatenated side-by-side. \n",
    "\n",
    "This is certainly not the only way to combine features from different transformers, but in my experience it is definitely one of the cleanest, especially when working with many types of hand-crafted features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "\n",
    "### Notebook Solutions\n",
    "***\n",
    "\n",
    "The success of a machine learning algorithm on a predictive problem is highly dependent on the way you as the practitioner present the data. Consider the following example. \n",
    "\n",
    "### Problem 1: Intuition\n",
    "***\n",
    "Suppose that you want to train a model to predict whether it is possible to drive between two cities in a single day.  The raw data includes the latitude and longitude of the two cities and the training data is labeled with $\\texttt{Yes}$ for is drivable and $\\texttt{No}$ for is not drivable.  One particular training set my look like \n",
    "\n",
    "|$\\texttt{CITY 1 LAT.}$ | $\\texttt{CITY 1 LNG.}$ |$\\texttt{CITY 2 LAT.}$ | $\\texttt{CITY 2 LNG.}$ | $\\texttt{DRIVABLE}$? | \n",
    "|:----:|:----:|:----:|:----:|:----:|\n",
    "| 123.24 |46.71\t| 121.33| 47.34\t| Yes |\n",
    "|123.24\t|56.91\t|121.33\t|55.23\t|Yes |\n",
    "|123.24\t|46.71\t|121.33\t|55.34\t|No |\n",
    "|123.24\t|46.71\t|130.99\t|47.34\t|No |\n",
    "\n",
    "**Q**: Given the following features, do you expect a linear classifier like Logistic Regression to be successful? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Probably not.  The issue here is that Logistic Regression will look at each of the features linearly and try to use a combination of each to make a prediction.  But notice that not a single one of the features is strongly correlated with the label.  It's clear that in order to make an accurate prediction the features have to interact, and they have to interact in a potentially complicated way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What features could you create that *would* be correlated with the correct classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Somehow we have to encode an idea of distance between the two cities in order to make a prediction.  The most obvious answer is to use the given features to create a derived feature that encodes distance, i.e. \n",
    "\n",
    "$$\n",
    "\\texttt{Distance} = \\sqrt{(\\texttt{CITY 1 LAT.} -\\texttt{CITY 2 LAT.})^2 + (\\texttt{CITY 2 LNG.} -\\texttt{CITY 1 LNG.})^2 }\n",
    "$$\n",
    "\n",
    "If we convert the following measurements to miles then we have the following table \n",
    "\n",
    "|$\\texttt{DISTANCE}$ | $\\texttt{DRIVABLE}$? | \n",
    "|:----:|:----:|:----:|:----:|:----:|\n",
    "| 14\t| Yes |\n",
    "| 28\t|Yes |\n",
    "| 705   |No |\n",
    "| 2432  |No |\n",
    "\n",
    "Clearly, the derived feature of distance is strongly correlated with drivablility and will thus be a good feature to include in the model (in fact, you probably want to use this feature and dump the lat-long features entirely). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Could we come up with a process to do this automatically?  Next question:  should we come up with a process to do this automatically? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**:  Meh... We could throw a bunch of derived polynomials terms into the mix and then do some feature selection method (we'll learn about Lasso next week) and see if it throws out the useless features.  But in this case we saved a lot of time by using common sense (i.e. domain knowledge). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 2: Transformations on Continuous Data \n",
    "***\n",
    "\n",
    "Consider the case when you're trying to model quality of life of a person from survey data which asks a multitude of things like $\\texttt{income}$, $\\texttt{education level}$, $\\texttt{num children}$, etc.  As part of data exploration, you might try plotting the distributions of the raw data by feature individually.  A histogram of a potential data set for income might look as follows:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAGVCAYAAADACFYXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103f98490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_inc = income_data()\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "n, bins, patches = plt.hist(x_inc, 15, normed=1, facecolor=mycolors[\"red\"], alpha=0.75)\n",
    "plt.xlabel(\"income in tens of thousands\", fontsize=16)\n",
    "plt.ylabel(\"frequency\", fontsize=16);\n",
    "plt.title(\"Income Frequency\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like we would expect for income.  There are lots of people at the low end of the spectrum, a few more in the middle, and very few in the high income categories.  \n",
    "\n",
    "There are two things to consider here.  First, lots of ML models assume that your input features are generally normally distributed.  Second, from a feature engineering perspective, we can think about how much income actually affects happiness.  It seems reasonable to believe that once you get to a certain level, increasing income has a diminishing effect on happiness.  \n",
    "\n",
    "Both of these viewpoints motivate us to try a lot transformation on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAt0AAAGVCAYAAAAi1E1uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmqmaFbkiRJqtn/D53547MrD+XqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103f3aad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_inc_log = np.log(x_inc)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "n, bins, patches = plt.hist(x_inc_log, 15, normed=1, facecolor=mycolors[\"blue\"], alpha=0.75)\n",
    "plt.xlabel(\"log(income in thousands)\", fontsize=16)\n",
    "plt.ylabel(\"frequency\", fontsize=16);\n",
    "plt.title(\"Income Frequency\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a distribution that looks fairly Gaussian, which might perform better in an machine learning model. \n",
    "\n",
    "### Problem 3: Categorical Data \n",
    "*** \n",
    "\n",
    "Encoding categorical data can be tricky.  Consider the case when you have a raw feature in your model that corresponds to a person's hair color.  Possible values might be $\\texttt{blonde}$, $\\texttt{brunette}$, $\\texttt{redhead}$.  How should we encode these as numerical models in a ML algorithm? \n",
    "\n",
    "A natural (but misleading) thing to do would be to assign an integer to each possible value of the feature.  For instance, we could do \n",
    "\n",
    "|Religion| Feature|\n",
    "|:-------:|:------:|\n",
    "|blonde| 0 |\n",
    "|brunette | 1 | \n",
    "|redhead | 2|\n",
    "\n",
    "**Q**:  What is potentially wrong with this encoding, particularly in a regression context? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: The problem is that we imposed on the possible values an ordinal ordering which does not exist, i.e. blonde > brunette > redhead.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to fix this is create binary features corresponding to each religion.  That is, we create a binary $\\texttt{IsBlonde}$ variable, a binary $\\texttt{IsBrunette}$ variable, etc.  We then have \n",
    "\n",
    "|Religion| $\\texttt{IsBlonde}$ | $\\texttt{IsBrunette}$ | $\\texttt{IsRedhead}$| \n",
    "|:-------:|:------:|:------:|:------:|\n",
    "|Blonde| 1 | 0 | 0 | \n",
    "|Brunette | 0 | 1 | 0 | \n",
    "| Redhead | 0| 0 | 1 \n",
    "\n",
    "This process is called *one-hot-encoding* and is very frequently used to encode categorical data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "\n",
    "### Problem 4: Temporal Data \n",
    "*** \n",
    "\n",
    "Suppose that you're trying to train a model to predict that amount of foot-traffic at the 29th Street Mall.  Mall managers might be interested in such a model to predict the amount of janitorial and security services they need to employ at different times.  \n",
    "\n",
    "Suppose your training data consists of measurements of the amount of foot traffic at the mall and the date/time stamps that they were measured.  For instance, a training set might look like \n",
    "\n",
    "|$\\texttt{date_time_stamp}$| $\\texttt{FootTraffic}$|\n",
    "|:-------:|:------:|\n",
    "|$\\texttt{2015-11-12-20:00}$| 70|\n",
    "|$\\texttt{2015-06-10-21:00}$| 100|\n",
    "|$\\texttt{2015-08-02-12:00}$| 120|\n",
    "|$\\texttt{2015-12-22-12:00}$| 20|\n",
    "\n",
    "\n",
    "**Q**: How might you create meaningful features on the $\\texttt{date_time_stamp}$ data that would be useful for prediction? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: You could do loads of things.  For starters you probably want to separate the dates and times.  Then you might create a feature like $\\texttt{isCloseToHoliday}$ or features that indicate which season it is (since foot traffic might go up during summer when all of the mall rats are out of school). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 5: Part-of-Speech-Tagging\n",
    "*This problem and associated code was adapted from Jordan Boyd-Graber*\n",
    "***\n",
    "\n",
    "In computational linguistics, part-of-speech tagging (POST) is the process of marking a word in a text as a particular part of speech (e.g. noun, verb, adjective, etc), based on both its definition and its context. \n",
    "\n",
    "In this problem we will work with the <a href=\"https://en.wikipedia.org/wiki/Brown_Corpus\">Brown Corpus</a>, a compilation of 500 samples of English-language text totaling over a million words. The Brown Corpus is available through python's <a href=\"http://www.nltk.org/\">Natural Language Toolkit</a>.  Each word in the corpus has been tagged as: \n",
    "\n",
    "|type|symbol|\n",
    "|:--:|:----:|\n",
    "|adjective| JJ|\n",
    "|noun|NN|\n",
    "|pronoun|PP|\n",
    "|adverb|RB|\n",
    "|verb|VB|\n",
    "\n",
    "For the classification we will use simple Logistic Regression and focus on making iterative improvements by adding good features to our model.  The code for this problem is located in the helper functions section below.  Scroll down now and take a look at the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Baseline**: When starting to evaluate the usefulness of features, it is usually a good idea to create a baseline.  That is, run your model with little to no features and see how the model performs.  The following code will run logistic regression with only a bias feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TRAIN\n",
      "-------------------------\n",
      "Accuracy: 0.526002\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "0\t522\t0\t0\t0\n",
      "0\t2205\t0\t0\t0\n",
      "0\t278\t0\t0\t0\n",
      "0\t179\t0\t0\t0\n",
      "0\t1008\t0\t0\t0\n",
      "(u'said', 'NN')\t74\n",
      "(u'he', 'NN')\t59\n",
      "(u'it', 'NN')\t40\n",
      "(u'his', 'NN')\t32\n",
      "(u'He', 'NN')\t24\n",
      "(u'its', 'NN')\t20\n",
      "(u'federal', 'NN')\t16\n",
      "(u'It', 'NN')\t16\n",
      "(u'new', 'NN')\t15\n",
      "(u'medical', 'NN')\t15\n",
      "Validation\n",
      "--------------------\n",
      "Accuracy: 0.560999\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "0\t129\t0\t0\t0\n",
      "0\t584\t0\t0\t0\n",
      "0\t59\t0\t0\t0\n",
      "0\t35\t0\t0\t0\n",
      "0\t234\t0\t0\t0\n",
      "(u'said', 'NN')\t21\n",
      "(u'he', 'NN')\t11\n",
      "(u'He', 'NN')\t9\n",
      "(u'made', 'NN')\t6\n",
      "(u'its', 'NN')\t5\n",
      "(u'his', 'NN')\t5\n",
      "(u'it', 'NN')\t5\n",
      "(u'It', 'NN')\t4\n",
      "(u'local', 'NN')\t4\n",
      "(u'their', 'NN')\t4\n"
     ]
    }
   ],
   "source": [
    "part_of_speech(limit=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now we know that simply making predictions based on the number of occurrences of each type of speech leads to a *training accuracy* of around 52% and a *validation accuracy* of around 56%.  Hopefully we can improve on this by actually giving the model some useful features.  \n",
    "\n",
    "The obvious choice is to actually tell the model what the words is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "County\n",
      "Grand\n",
      "Jury\n",
      "said\n",
      "investigation\n",
      "recent\n",
      "primary\n",
      "election\n",
      "produced\n",
      "evidence\n",
      "irregularities\n",
      "took\n",
      "place\n",
      "TRAIN\n",
      "-------------------------\n",
      "Accuracy: 0.962309\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "515\t6\t0\t0\t1\n",
      "12\t2186\t1\t1\t5\n",
      "0\t9\t269\t0\t0\n",
      "6\t75\t0\t98\t0\n",
      "2\t38\t0\t2\t966\n",
      "(u'public', 'JJ')\t4\n",
      "(u'increase', 'NN')\t4\n",
      "(u'back', 'RB')\t3\n",
      "(u'pay', 'VB')\t2\n",
      "(u'report', 'NN')\t2\n",
      "(u'further', 'JJ')\t2\n",
      "(u'Executive', 'NN')\t2\n",
      "(u'better', 'JJ')\t2\n",
      "(u'plans', 'NN')\t2\n",
      "(u'place', 'NN')\t2\n",
      "Validation\n",
      "--------------------\n",
      "Accuracy: 0.792507\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "63\t65\t0\t1\t0\n",
      "4\t574\t0\t0\t6\n",
      "0\t6\t53\t0\t0\n",
      "2\t19\t0\t14\t0\n",
      "0\t113\t0\t0\t121\n",
      "(u'future', 'NN')\t2\n",
      "(u'you', 'NN')\t2\n",
      "(u'certain', 'NN')\t2\n",
      "(u'past', 'NN')\t2\n",
      "(u'agreed', 'NN')\t2\n",
      "(u'permitting', 'NN')\t2\n",
      "(u'lacking', 'NN')\t2\n",
      "(u'going', 'NN')\t2\n",
      "(u'North', 'NN')\t2\n",
      "(u'getting', 'NN')\t2\n"
     ]
    }
   ],
   "source": [
    "part_of_speech(limit=500, word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that seems more reasonable.  Now you should have a training accuracy of around 96% and a validation accuracy of around 79%.  It looks like using the words alone as features induces some overfitting on the training set. Let's see if we can think of new features that we could include that might help.  \n",
    "\n",
    "Here is where you have to use your brain to do feature engineering!  \n",
    "\n",
    "The code has output some useful information that you can use to do error analysis and hopefully come up with some useful features.  \n",
    "\n",
    "The top of the output gives you examples of the features that were used for several examples.  Since we only used the words as features, each example only includes the word itself. \n",
    "\n",
    "The next useful piece of information is shown in the *confusion matrix*. Sci-Kit Learn's confusion matrix function returns a matric $C$ such that $C_{ij}$ gives the number of examples known to be in group $i$ that were labeled as group $j$.  From the confusion matrix in the output we see that, in particular, the model is classifying a lot of words that should be verbs as nouns.   \n",
    "\n",
    "Now, based on this knowledge you might have some ideas about new features you can include to combat this error, but maybe you don't?  It's almost always a good idea to dig into the actual data and look at *specific* examples that your model has misclassified.  To help you with this, the code has printed some of the most common misclassifications.  \n",
    "\n",
    "**Q**: Looking at the common misclassifications, can you think of a good new feature to add? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your new feature to the model and see how it does! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "County N:Grand\n",
      "Grand N:Jury\n",
      "Jury N:said\n",
      "said N:Friday\n",
      "investigation N:of\n",
      "recent N:primary\n",
      "primary N:election\n",
      "election N:produced\n",
      "produced N:``\n",
      "evidence N:''\n",
      "irregularities N:took\n",
      "took N:place\n",
      "place N:.\n",
      "TRAIN\n",
      "-------------------------\n",
      "Accuracy: 0.927242\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "471\t47\t0\t0\t4\n",
      "2\t2179\t1\t0\t23\n",
      "0\t17\t259\t0\t2\n",
      "2\t88\t0\t78\t11\n",
      "1\t107\t0\t0\t900\n",
      "(u'back', 'NN')\t4\n",
      "(u'pay', 'VB')\t2\n",
      "(u'locally', 'NN')\t2\n",
      "(u'previously', 'NN')\t2\n",
      "(u'me', 'NN')\t2\n",
      "(u'further', 'NN')\t2\n",
      "(u'themselves', 'NN')\t2\n",
      "(u'him', 'NN')\t2\n",
      "(u'increase', 'NN')\t2\n",
      "(u'general', 'NN')\t2\n",
      "Validation\n",
      "--------------------\n",
      "Accuracy: 0.791547\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "51\t76\t0\t1\t1\n",
      "2\t570\t0\t0\t12\n",
      "0\t8\t49\t0\t2\n",
      "1\t22\t0\t7\t5\n",
      "0\t87\t0\t0\t147\n",
      "(u'future', 'NN')\t2\n",
      "(u'agreed', 'NN')\t2\n",
      "(u'you', 'NN')\t2\n",
      "(u'His', 'NN')\t2\n",
      "(u'critical', 'NN')\t2\n",
      "(u'past', 'NN')\t2\n",
      "(u'permitting', 'NN')\t2\n",
      "(u'lacking', 'NN')\t2\n",
      "(u'fact', 'VB')\t2\n",
      "(u'Foreign', 'NN')\t2\n"
     ]
    }
   ],
   "source": [
    "part_of_speech(limit=500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**:  Did your model do better on the test data?  Take a look at the confusion matrix and the examples of misclassifications and see if you can think of another new feature to add! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:Fulton N:Grand D:NOUN D:NOUN C:~C C:Co C:ou C:un C:nt C:ty C:y^ C:~Co C:Cou C:oun C:unt C:nty C:ty^ C:~Cou C:Coun C:ount C:unty C:nty^\n",
      "P:County N:Jury D:ADJ D:ADJ D:ADJ D:ADJ D:ADJ D:ADJ D:ADJ D:ADJ D:NOUN D:NOUN C:~G C:Gr C:ra C:an C:nd C:d^ C:~Gr C:Gra C:ran C:and C:nd^ C:~Gra C:Gran C:rand C:and^\n",
      "P:Grand N:said D:NOUN D:NOUN C:~J C:Ju C:ur C:ry C:y^ C:~Ju C:Jur C:ury C:ry^ C:~Jur C:Jury C:ury^\n",
      "P:Jury N:Friday D:ADJ D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB C:~s C:sa C:ai C:id C:d^ C:~sa C:sai C:aid C:id^ C:~sai C:said C:aid^\n",
      "P:an N:of D:NOUN D:NOUN C:~i C:in C:nv C:ve C:es C:st C:ti C:ig C:ga C:at C:ti C:io C:on C:n^ C:~in C:inv C:nve C:ves C:est C:sti C:tig C:iga C:gat C:ati C:tio C:ion C:on^ C:~inv C:inve C:nves C:vest C:esti C:stig C:tiga C:igat C:gati C:atio C:tion C:ion^\n",
      "P:Atlanta's N:primary D:ADJ D:ADJ D:NOUN C:~r C:re C:ec C:ce C:en C:nt C:t^ C:~re C:rec C:ece C:cen C:ent C:nt^ C:~rec C:rece C:ecen C:cent C:ent^\n",
      "P:recent N:election D:ADJ D:ADJ D:ADJ D:ADJ D:ADJ D:NOUN D:NOUN D:NOUN D:NOUN C:~p C:pr C:ri C:im C:ma C:ar C:ry C:y^ C:~pr C:pri C:rim C:ima C:mar C:ary C:ry^ C:~pri C:prim C:rima C:imar C:mary C:ary^\n",
      "P:primary N:produced D:NOUN D:NOUN D:NOUN D:NOUN C:~e C:el C:le C:ec C:ct C:ti C:io C:on C:n^ C:~el C:ele C:lec C:ect C:cti C:tio C:ion C:on^ C:~ele C:elec C:lect C:ecti C:ctio C:tion C:ion^\n",
      "P:election N:`` D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB C:~p C:pr C:ro C:od C:du C:uc C:ce C:ed C:d^ C:~pr C:pro C:rod C:odu C:duc C:uce C:ced C:ed^ C:~pro C:prod C:rodu C:oduc C:duce C:uced C:ced^\n",
      "P:no N:'' D:VERB D:VERB D:VERB D:NOUN D:NOUN D:NOUN C:~e C:ev C:vi C:id C:de C:en C:nc C:ce C:e^ C:~ev C:evi C:vid C:ide C:den C:enc C:nce C:ce^ C:~evi C:evid C:vide C:iden C:denc C:ence C:nce^\n",
      "P:any N:took D:NOUN D:NOUN D:NOUN D:NOUN C:~i C:ir C:rr C:re C:eg C:gu C:ul C:la C:ar C:ri C:it C:ti C:ie C:es C:s^ C:~ir C:irr C:rre C:reg C:egu C:gul C:ula C:lar C:ari C:rit C:iti C:tie C:ies C:es^ C:~irr C:irre C:rreg C:regu C:egul C:gula C:ular C:lari C:arit C:riti C:itie C:ties C:ies^\n",
      "P:irregularities N:place D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB C:~t C:to C:oo C:ok C:k^ C:~to C:too C:ook C:ok^ C:~too C:took C:ook^\n",
      "P:took N:. D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN C:~p C:pl C:la C:ac C:ce C:e^ C:~pl C:pla C:lac C:ace C:ce^ C:~pla C:plac C:lace C:ace^\n",
      "TRAIN\n",
      "-------------------------\n",
      "Accuracy: 0.948950\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "471\t25\t0\t6\t20\n",
      "19\t2137\t0\t0\t49\n",
      "0\t2\t276\t0\t0\n",
      "12\t5\t0\t162\t0\n",
      "7\t69\t0\t0\t932\n",
      "(u'present', 'VB')\t6\n",
      "(u'meeting', 'VB')\t5\n",
      "(u'back', 'NN')\t4\n",
      "(u'increase', 'NN')\t4\n",
      "(u'East', 'NN')\t4\n",
      "(u'public', 'JJ')\t4\n",
      "(u'later', 'JJ')\t3\n",
      "(u'mention', 'NN')\t3\n",
      "(u'act', 'VB')\t3\n",
      "(u'amount', 'NN')\t3\n",
      "Validation\n",
      "--------------------\n",
      "Accuracy: 0.928915\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "104\t15\t0\t3\t7\n",
      "6\t560\t0\t0\t18\n",
      "0\t1\t58\t0\t0\n",
      "3\t2\t0\t30\t0\n",
      "1\t18\t0\t0\t215\n",
      "(u'meeting', 'VB')\t3\n",
      "(u'present', 'VB')\t3\n",
      "(u'further', 'RB')\t2\n",
      "(u'work', 'VB')\t2\n",
      "(u'North', 'NN')\t2\n",
      "(u'visit', 'VB')\t1\n",
      "(u'con', 'NN')\t1\n",
      "(u'use', 'NN')\t1\n",
      "(u'meetings', 'VB')\t1\n",
      "(u'Even', 'JJ')\t1\n"
     ]
    }
   ],
   "source": [
    "part_of_speech(limit=500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**:  What's your performance like now?  Repeat this iterative process of adding new features until you're happy with your model (or you've exhausted the number of features available in the code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:~C C:Co C:ou C:un C:nt C:ty C:y^ C:~Co C:Cou C:oun C:unt C:nty C:ty^ C:~Cou C:Coun C:ount C:unty C:nty^\n",
      "C:~G C:Gr C:ra C:an C:nd C:d^ C:~Gr C:Gra C:ran C:and C:nd^ C:~Gra C:Gran C:rand C:and^\n",
      "C:~J C:Ju C:ur C:ry C:y^ C:~Ju C:Jur C:ury C:ry^ C:~Jur C:Jury C:ury^\n",
      "C:~s C:sa C:ai C:id C:d^ C:~sa C:sai C:aid C:id^ C:~sai C:said C:aid^\n",
      "C:~i C:in C:nv C:ve C:es C:st C:ti C:ig C:ga C:at C:ti C:io C:on C:n^ C:~in C:inv C:nve C:ves C:est C:sti C:tig C:iga C:gat C:ati C:tio C:ion C:on^ C:~inv C:inve C:nves C:vest C:esti C:stig C:tiga C:igat C:gati C:atio C:tion C:ion^\n",
      "C:~r C:re C:ec C:ce C:en C:nt C:t^ C:~re C:rec C:ece C:cen C:ent C:nt^ C:~rec C:rece C:ecen C:cent C:ent^\n",
      "C:~p C:pr C:ri C:im C:ma C:ar C:ry C:y^ C:~pr C:pri C:rim C:ima C:mar C:ary C:ry^ C:~pri C:prim C:rima C:imar C:mary C:ary^\n",
      "C:~e C:el C:le C:ec C:ct C:ti C:io C:on C:n^ C:~el C:ele C:lec C:ect C:cti C:tio C:ion C:on^ C:~ele C:elec C:lect C:ecti C:ctio C:tion C:ion^\n",
      "C:~p C:pr C:ro C:od C:du C:uc C:ce C:ed C:d^ C:~pr C:pro C:rod C:odu C:duc C:uce C:ced C:ed^ C:~pro C:prod C:rodu C:oduc C:duce C:uced C:ced^\n",
      "C:~e C:ev C:vi C:id C:de C:en C:nc C:ce C:e^ C:~ev C:evi C:vid C:ide C:den C:enc C:nce C:ce^ C:~evi C:evid C:vide C:iden C:denc C:ence C:nce^\n",
      "C:~i C:ir C:rr C:re C:eg C:gu C:ul C:la C:ar C:ri C:it C:ti C:ie C:es C:s^ C:~ir C:irr C:rre C:reg C:egu C:gul C:ula C:lar C:ari C:rit C:iti C:tie C:ies C:es^ C:~irr C:irre C:rreg C:regu C:egul C:gula C:ular C:lari C:arit C:riti C:itie C:ties C:ies^\n",
      "C:~t C:to C:oo C:ok C:k^ C:~to C:too C:ook C:ok^ C:~too C:took C:ook^\n",
      "C:~p C:pl C:la C:ac C:ce C:e^ C:~pl C:pla C:lac C:ace C:ce^ C:~pla C:plac C:lace C:ace^\n",
      "TRAIN\n",
      "-------------------------\n",
      "Accuracy: 0.917462\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "423\t80\t0\t5\t14\n",
      "17\t2149\t1\t2\t36\n",
      "0\t2\t276\t0\t0\n",
      "8\t23\t0\t146\t2\n",
      "3\t151\t0\t2\t852\n",
      "(u'back', 'NN')\t6\n",
      "(u'meeting', 'VB')\t5\n",
      "(u'public', 'JJ')\t4\n",
      "(u'need', 'VB')\t4\n",
      "(u'increase', 'NN')\t4\n",
      "(u'better', 'NN')\t4\n",
      "(u'mean', 'NN')\t3\n",
      "(u'meet', 'NN')\t3\n",
      "(u'mention', 'NN')\t3\n",
      "(u'own', 'NN')\t3\n",
      "Validation\n",
      "--------------------\n",
      "Accuracy: 0.847262\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "73\t49\t0\t1\t6\n",
      "10\t550\t0\t2\t22\n",
      "0\t4\t55\t0\t0\n",
      "2\t7\t0\t23\t3\n",
      "0\t53\t0\t0\t181\n",
      "(u'meeting', 'VB')\t3\n",
      "(u'approval', 'JJ')\t3\n",
      "(u'you', 'NN')\t2\n",
      "(u'mind', 'VB')\t2\n",
      "(u'certain', 'NN')\t2\n",
      "(u'blue', 'NN')\t2\n",
      "(u'past', 'NN')\t2\n",
      "(u'meet', 'NN')\t2\n",
      "(u'future', 'NN')\t2\n",
      "(u'need', 'VB')\t2\n"
     ]
    }
   ],
   "source": [
    "part_of_speech(limit=500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 6: Building Multiple Features in SKLearn with Custom Transformers and FeatureUnion\n",
    "***\n",
    "\n",
    "In the Feature Engineering assignment your goal is to hand-craft features to predict whether statements about TV shows contain spoilers or not.  There are endless possibilities for useful features that you might want to try out, but for those of you not familiar with text-learning in sklearn just the process of getting up and running might seem daunting.  In this problem I will demonstrate the use of two particular objects that may make your life significantly easier: the generic Transformer and the FeatureUnion pipeline.  We will also make use of the CountVectorizer which is similar to the HashVectorizer seen in Problem 5.  If you have significant experience in text-learning then you have likely seen these things before (and/or know better things) and can safely skip this exercise. \n",
    "\n",
    "For the purpose of this discussion we will assume, like in the homework, that our data is a list of sentence strings.  For instance, we might have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [\n",
    "    \"The quick brown fox jumped over the lazy lazy dog\",\n",
    "    \"There is that dog and fox again\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to turn these data into a vectorized bag-of-word representation we can use an instance of CountVectorizer, which is a simple transformer that turns raw text into bag-or-words vectors.  To make the number of word-features more manageable I will call CountVectorizer with a list of stop words to be removed.  \n",
    "\n",
    "To transform the data into a matrix I simply call the $\\texttt{fit_transform}$ method on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words = CountVectorizer(stop_words='english')\n",
    "X = bag_of_words.fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to see the names of the specific features created (and important for our purpose, the order in which they're encoded) I can call the $\\texttt{get_feature_names}$ method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The named features are \", bag_of_words.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that CountVectorizer removed most of the common words like \"the\", \"there\", \"is\" and \"and\" and stores the important words in alphabetical order. \n",
    "\n",
    "Let's check the type and shape of the matrix produced by the transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"X has type \", type(X)\n",
    "print \"X has shape \", X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So CountVectorizer returns our vectorized features as a sparse matrix.  This makes sense since most feature vectors in text applications only contain a handful of vocabulary words and are therefore very sparse. \n",
    "\n",
    "Finally, the matrix X has two rows and six columns.  Each row corresponds to a training example (a sentence) and each column to a word-feature with the order corresponding to the list returned by $\\texttt{get_feature_names}$. \n",
    "\n",
    "Let's look at the matrix and see if it does what we think it's doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row of X should refer to the first sentence in $\\texttt{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first column refers to instances of \"brown\", and the fifth column refers to instances of \"lazy\".  OK, so there's an example of a simple canned transformer that we can apply to raw text-data.  But what if we want to craft something a little less standard?  What if, for instance, we're convinced that the number of times the characters \"x\", \"y\", and \"z\" appear in a sentence is somehow an important feature (probably not, but just go with it).  How could add these features to our data matrix?   \n",
    "\n",
    "There are definitely simple hacky ways to do this, but one slick way is to write your own custom transformer.  This transformer will take in raw text-data, turn them into numeric feature vectors (counts of the number of \"x\", \"y\", and \"z\"s) and return the matrix of transformed data.  One such transformer might look as follows  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class XYZTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        letters = ['x', 'y', 'z']\n",
    "         \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), len(letters)))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii,:] = np.array([x.count(letter) for letter in letters])\n",
    "            \n",
    "        return csr_matrix(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially we're writing a class that takes in the type of data we expect, transforms it the way we expect, and returns it the way we expect.  One thing to note is that by convention the transformer contains a $\\texttt{fit}$ method that only results $\\texttt{self}$.  All of the magic actually happens in the $\\texttt{transform}$ method. \n",
    "\n",
    "Let's test it out on our simple training data and see if it does what we expect.  Remember that the sentences in the training data are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train[0]\n",
    "print train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of the XYZTransformer \n",
    "xyz = XYZTransformer()\n",
    "\n",
    "# Fit it to our data \n",
    "Y = xyz.fit_transform(train)\n",
    "\n",
    "# Print a dense version of the matrix \n",
    "print Y.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first row (corresponding to the first sentence in the data) indicates that there is 1 x, 2 y's, and 2 z's in the first sentence, which you can verify is correct.  Similarly, the second row indicates that there is a single x (from \"fox\") in the sentence and no y's or z's. \n",
    "\n",
    "OK, so you've built your first transformer.  Now, let's say we want to combine the bag-of-words vectors and the letter features into a single data matrix.  We can do this with a particular class called a $\\texttt{FeatureUnion}$.  If you're familiar with sklearn's pipelines, know that a $\\texttt{FeatureUnion}$ is a pipeline specifically designed for transformer objects. \n",
    "\n",
    "Now we'll combine our two transformers into a mega-transformer that will zip all of our features into a nice package.  It might look as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "allmyfeatures = FeatureUnion([\n",
    "        (\"bag-of-words\", CountVectorizer(stop_words='english')),\n",
    "        (\"letter-counts\", XYZTransformer())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll call our mega-transformer on the original data, and hopefully get a sparse matrix out that encapsulates all of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = allmyfeatures.fit_transform(train)\n",
    "print \"Z has type \", type(Z)\n",
    "print \"Z has shape \", Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the combined data matrix Z is again a csr matrix (by design) and has two rows and nine columns.  The two rows again correspond to the two pieces of data in the train set, and the nine columns correspond to the six bag-of-word features from CountVectorizer and the three letter count features from XYZTransformer.  If we print a dense version of Z ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print Z.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will see that it looks exactly like the data matrices X and Y concatenated side-by-side. \n",
    "\n",
    "This is certainly not the only way to combine features from different transformers, but in my experience it is definitely one of the cleanest, especially when working with many types of hand-crafted features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "<a id='helpers'></a>\n",
    "\n",
    "<br> \n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cketelsen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/cketelsen/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import string\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import seaborn as sn \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "\n",
    "\n",
    "def normalize_tags(tag):\n",
    "    if not tag or not tag[0] in string.uppercase:\n",
    "        return \"PUNC\"\n",
    "    else:\n",
    "        return tag[:2]\n",
    "\n",
    "\n",
    "kTAGSET = [\"\", \"JJ\", \"NN\", \"PP\", \"RB\", \"VB\"]\n",
    "\n",
    "class Analyzer:\n",
    "    def __init__(self, word, before, after, prev, next, char, dict):\n",
    "        self.word = word\n",
    "        self.after = after\n",
    "        self.before = before\n",
    "        self.prev = prev\n",
    "        self.next = next\n",
    "        self.dict = dict\n",
    "        self.char = char\n",
    "\n",
    "    def __call__(self, feature_string):\n",
    "        feats = feature_string.split()\n",
    "\n",
    "        if self.word:\n",
    "            yield feats[0]\n",
    "\n",
    "        if self.after:\n",
    "            for ii in [x for x in feats if x.startswith(\"A:\")]:\n",
    "                yield ii\n",
    "        if self.before:\n",
    "            for ii in [x for x in feats if x.startswith(\"B:\")]:\n",
    "                yield ii\n",
    "        if self.prev:\n",
    "            for ii in [x for x in feats if x.startswith(\"P:\")]:\n",
    "                yield ii\n",
    "        if self.next:\n",
    "            for ii in [x for x in feats if x.startswith(\"N:\")]:\n",
    "                yield ii\n",
    "        if self.dict:\n",
    "            for ii in [x for x in feats if x.startswith(\"D:\")]:\n",
    "                yield ii\n",
    "        if self.char:\n",
    "            for ii in [x for x in feats if x.startswith(\"C:\")]:\n",
    "                yield ii\n",
    "                \n",
    "def example(sentence, position):\n",
    "        word = sentence[position][0]\n",
    "        ex = word\n",
    "        tag = normalize_tags(sentence[position][1])\n",
    "        if tag in kTAGSET:\n",
    "            target = kTAGSET.index(tag)\n",
    "        else:\n",
    "            target = None\n",
    "\n",
    "        if position > 0:\n",
    "            prev = \" P:%s\" % sentence[position - 1][0]\n",
    "        else:\n",
    "            prev = \"\"\n",
    "\n",
    "        if position < len(sentence) - 1:\n",
    "            next = \" N:%s\" % sentence[position + 1][0]\n",
    "        else:\n",
    "            next = ''\n",
    "\n",
    "        all_before = \" \" + \" \".join([\"B:%s\" % x[0]\n",
    "                                     for x in sentence[:position]])\n",
    "        all_after = \" \" + \" \".join([\"A:%s\" % x[0]\n",
    "                                    for x in sentence[(position + 1):]])\n",
    "\n",
    "        dictionary = [\"D:ADJ\"] * len(wn.synsets(word, wn.ADJ)) + \\\n",
    "          [\"D:ADV\"] * len(wn.synsets(word, wn.ADV)) + \\\n",
    "          [\"D:VERB\"] * len(wn.synsets(word, wn.VERB)) + \\\n",
    "          [\"D:NOUN\"] * len(wn.synsets(word, wn.NOUN))\n",
    "\n",
    "        dictionary = \" \" + \" \".join(dictionary)\n",
    "\n",
    "        char = ' '\n",
    "        padded_word = \"~%s^\" % sentence[position][0]\n",
    "        for ngram_length in xrange(2, 5):\n",
    "            char += ' ' + \" \".join(\"C:%s\" % \"\".join(cc for cc in x)\n",
    "                                   for x in ngrams(padded_word, ngram_length))\n",
    "        ex += char\n",
    "\n",
    "        ex += prev\n",
    "        ex += next\n",
    "        ex += all_after\n",
    "        ex += all_before\n",
    "        ex += dictionary\n",
    "\n",
    "        return ex, target\n",
    "    \n",
    "def all_examples(limit, train=True):\n",
    "    sent_num = 0\n",
    "    for ii in brown.tagged_sents():\n",
    "        sent_num += 1\n",
    "        if limit > 0 and sent_num > limit:\n",
    "            break\n",
    "\n",
    "        for jj in xrange(len(ii)):\n",
    "            ex, tgt = example(ii, jj)\n",
    "            if tgt:\n",
    "                if train and sent_num % 5 != 0:\n",
    "                    yield ex, tgt\n",
    "                if not train and sent_num % 5 == 0:\n",
    "                    yield ex, tgt\n",
    "                    \n",
    "def accuracy(classifier, x, y, examples):\n",
    "    predictions = classifier.predict(x)\n",
    "    cm = confusion_matrix(y, predictions)\n",
    "\n",
    "    print(\"Accuracy: %f\" % accuracy_score(y, predictions))\n",
    "\n",
    "    print(\"\\t\".join(kTAGSET[1:]))\n",
    "    for ii in cm:\n",
    "        print(\"\\t\".join(str(x) for x in ii))\n",
    "\n",
    "    errors = defaultdict(int)\n",
    "    for ii, ex_tuple in enumerate(examples):\n",
    "        ex, tgt = ex_tuple\n",
    "        if tgt != predictions[ii]:\n",
    "            errors[(ex.split()[0], kTAGSET[predictions[ii]])] += 1\n",
    "\n",
    "    for ww, cc in sorted(errors.items(), key=operator.itemgetter(1),\n",
    "                         reverse=True)[:10]:\n",
    "        print(\"%s\\t%i\" % (ww, cc))\n",
    "        \n",
    "def part_of_speech(**kwargs):\n",
    "    word = kwargs.get(\"word\", False)\n",
    "    all_before = kwargs.get(\"all_before\", False)\n",
    "    all_after = kwargs.get(\"all_after\", False)\n",
    "    one_before = kwargs.get(\"one_before\", False)\n",
    "    one_after = kwargs.get(\"one_after\", False)\n",
    "    characters = kwargs.get(\"characters\", False)\n",
    "    dictionary = kwargs.get(\"dictionary\", False)\n",
    "    limit= kwargs.get(\"limit\",-1)\n",
    "    \n",
    "    analyzer = Analyzer(word, all_before, all_after,\n",
    "                        one_before, one_after, characters,\n",
    "                        dictionary)\n",
    "    \n",
    "    vectorizer = HashingVectorizer(analyzer=analyzer)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(ex for ex, tgt in\n",
    "                                       all_examples(limit))\n",
    "    x_valid = vectorizer.fit_transform(ex for ex, tgt in\n",
    "                                      all_examples(limit, train=False))\n",
    "    \n",
    "    for ex, tgt in all_examples(1):\n",
    "        print(\" \".join(analyzer(ex)))\n",
    "\n",
    "    y_train = np.array(list(tgt for ex, tgt in all_examples(limit)))\n",
    "    y_valid = np.array(list(tgt for ex, tgt in\n",
    "                        all_examples(limit, train=False)))\n",
    "\n",
    "    lr = SGDClassifier(loss='log', penalty='l2', shuffle=True)\n",
    "    lr.fit(x_train, y_train)\n",
    "\n",
    "    print(\"TRAIN\\n-------------------------\")\n",
    "    accuracy(lr, x_train, y_train, all_examples(limit))\n",
    "    print(\"Validation\\n--------------------\")\n",
    "    accuracy(lr, x_valid, y_valid, all_examples(limit, train=False))\n",
    "    \n",
    "np.random.seed(1234)\n",
    "    \n",
    "def income_data(N=200):\n",
    "    x = 1.1*np.random.normal(size=N)\n",
    "    y = np.exp(x)\n",
    "    return y\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
